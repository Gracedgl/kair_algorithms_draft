# ! usr/bin/env python

import time

import gym
import numpy as np
from gym import spaces
from gym.utils import seeding

import rospy
from gazebo_msgs.srv import GetModelState
from ros_interface import OpenManipulatorRosInterface

# Global variables
ACTION_DIM = 3  # Cartesian
OBSERVATION_DIM = 4
X_MIN = 0.1
X_MAX = 0.5
Y_MIN = -0.3
Y_MAX = 0.3
Z_MIN = 0.0
Z_MAX = 0.6
TERM_COUNT = 10
SUC_COUNT = 10


class OpenManipulatorReacherEnv(gym.Env):
    def __init__(
            self,
            max_steps=700,
            isdagger=False,
            isPOMDP=False,
            isreal=False,
            train_indicator=0,
    ):

        self.action_space = spaces.Box(-1.0, 1.0, shape=(ACTION_DIM,), dtype="float32")
        self.observation_space = spaces.Dict(
            dict(
                observation=spaces.Box(
                    -np.inf, np.inf, shape=(OBSERVATION_DIM,),
                    dtype="float32"
                )
            )
        )
        self.ros_interface = OpenManipulatorRosInterface()
        self.train_indicator = train_indicator
        self.isdagger = isdagger
        self.isPOMDP = isPOMDP
        self.isreal = isreal
        self.max_steps = max_steps
        self.done = False
        self.reward = 0
        self.reward_rescale = 1.0
        self.reward_type = "sparse"
        self.termination_count = 0
        self.success_count = 0
        self.step_cnt = 0
        self.tic = 0.0
        self.toc = 0.0
        self.elapsed = 0.0

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def step(self, action):
        """Function executed each time step.
        Here we get the action execute it in a time step and retrieve the
        observations generated by that action.
        :param action:
        :return: obs, reward, done
        """
        if action is None:
            action = np.array([1, 1, 1, 1, 1, 1])
        self.prev_tic = self.tic
        self.tic = time.time()
        self.elapsed = time.time() - self.prev_tic
        self.done = False
        if self.step_cnt == self.max_steps:
            self.done = True
            self.step_cnt = 0
        act = action.flatten().tolist()
        self.ros_interface.set_joints_position(act)
        curDist = self._get_dist()
        if not self.isreal:
            self.reward = self._compute_reward()
            if self._check_for_termination():
                print("======================================================")
                print("Terminates current Episode : OUT OF BOUNDARY")
                print("======================================================")
            elif self._check_for_success():
                print("======================================================")
                print("Succeeded current Episode")
                print("======================================================")
        _joint_pos, _joint_vels, _joint_effos = self.ros_interface.get_joints_states()
        # obj_pos = self._get_target_obj_obs() # TODO: implement this
        #  function call.

        if np.mod(self.step_cnt, 10) == 0:
            if not self.isreal:
                print("DISTANCE : ", curDist)
            print("PER STEP ELAPSED : ", self.elapsed)
            print("SPARSE REWARD : ", self.reward_rescale * self.reward)
            print("Current EE pos: ", self.ros_interface.gripper_position)
            print("Actions: ", act)

        obs = np.array([_joint_pos, _joint_vels, _joint_effos])
        self.step_cnt += 1

        return obs, self.reward_rescale * self.reward, self.done

    def reset(self):
        """Attempt to reset the simulator. Since we randomize initial
        conditions, it
        is possible to get into a state with numerical issues (e.g. due to
        penetration or
        Gimbel lock) or we may not achieve an initial condition (e.g. an
        object is within the hand).
        In this case, we just keep randomizing until we eventually achieve
        a valid initial
        configuration."""
        # did_reset_sim = False
        self.ros_interface._reset_gazebo_world()
        _joint_pos, _joint_vels, _joint_effos = self.ros_interface.get_joints_states()
        obs = np.array([_joint_pos, _joint_vels, _joint_effos])

        return obs

    def close(self):
        rospy.signal_shutdown("done")

    def check_robot_moving(self):
        """Check if robot has reached its initial pose.
        """
        while not rospy.is_shutdown():
            if self.moving_state == "STOPPED":
                break
        return True

    def _check_for_success(self):
        """Check if the agent has succeeded the episode.
        """
        _dist = self._get_dist()
        if _dist < self.ros_interface.distance_threshold:
            self.success_count += 1
            if self.success_count == SUC_COUNT:
                self.done = True
                self.success_count = 0
                return True
        else:
            return False

    def _check_for_termination(self):
        """Check if the agent has reached undesirable state. If so, terminate
        the episode early.
        """
        _ee_pose = self.ros_interface.get_gripper_position()
        if not (
                (X_MIN < _ee_pose[0] < X_MAX) and (Y_MIN < _ee_pose[1] < Y_MAX) and (
                Z_MIN < _ee_pose[1] < Z_MAX)):
            self.termination_count += 1
        if self.termination_count == TERM_COUNT:
            self.done = True
            self.termination_count = 0
            return True
        else:
            return False

    def _compute_reward(self):
        """Computes shaped/sparse reward for each episode.
        """
        cur_dist = self._get_dist()
        if self.reward_type == "sparse":
            return (cur_dist <= self.ros_interface.distance_threshold).astype(
                np.float32
            )  # 1 for success else 0
        else:
            return -cur_dist - self.squared_sum_vel  # -L2 distance
            # -l2_norm(joint_vels)

    def _get_dist(self):
        rospy.wait_for_service("/gazebo/get_model_state")
        try:
            object_state_srv = rospy.ServiceProxy(
                "/gazebo/get_model_state", GetModelState
            )
            object_state = object_state_srv("block", "world")
            self._obj_pose = np.array(
                [
                    object_state.pose.position.x,
                    object_state.pose.position.y,
                    object_state.pose.position.z,
                ]
            )
        except rospy.ServiceException as e:
            rospy.logerr("Spawn URDF service call failed: {0}".format(e))
        _ee_pose = np.array(
            self.ros_interface.get_gripper_position())  # FK state of robot
        return np.linalg.norm(_ee_pose - self._obj_pose)
